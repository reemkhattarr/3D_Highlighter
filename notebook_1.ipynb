{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yapv-ugsPssd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/reemkhattarr/3D_Highlighter\n",
        "%cd 3D_Highlighter\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install git+https://github.com/NVIDIAGameWorks/kaolin.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puUkOXi-Psse"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from itertools import permutations, product\n",
        "from Normalization import MeshNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "\n",
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self, depth, width, out_dim, input_dim=3, sigma=5.0):\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, width))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.LayerNorm([width]))\n",
        "        for i in range(depth):\n",
        "            layers.append(nn.Linear(width, width))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm([width]))\n",
        "        layers.append(nn.Linear(width, out_dim))\n",
        "        layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "        self.mlp = nn.ModuleList(layers)\n",
        "        print(self.mlp)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.mlp:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "def get_clip_model(clipmodel):\n",
        "    clip_model, preprocess = clip.load(clipmodel, device)\n",
        "    return clip_model, preprocess\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
        "                                                                        show=False,\n",
        "                                                                        center_azim=0,\n",
        "                                                                        center_elev=0,\n",
        "                                                                        std=1,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name+'.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(rendered_images, encoded_text, clip_model, n_augs):\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    loss = 0.0\n",
        "    for _ in range(n_augs):\n",
        "        augmented_image = augment_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(augmented_image)\n",
        "        if encoded_text.shape[0] > 1:\n",
        "            loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                            torch.mean(encoded_text, dim=0), dim=0)\n",
        "        else:\n",
        "            loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                            encoded_text)\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOujv7iUPssf"
      },
      "outputs": [],
      "source": [
        "# Constrain most sources of randomness\n",
        "# (some torch backwards functions within CLIP are non-determinstic)\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "render_res = 224\n",
        "n_iter = 2500\n",
        "res = 224\n",
        "\n",
        "def optimize(obj_path='data/candle.obj', prompt='a gray candle with highlighted hat.',\n",
        "             learning_rate=0.0001, depth=4, n_augs=5, n_views=5):\n",
        "  output_dir = './output/'\n",
        "  clip_model = 'ViT-L/14'\n",
        "\n",
        "  Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "  render = Renderer(dim=(render_res, render_res))\n",
        "  mesh = Mesh(obj_path)\n",
        "  MeshNormalizer(mesh)()\n",
        "\n",
        "  # Initialize variables\n",
        "  background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "  log_dir = output_dir\n",
        "\n",
        "\n",
        "  # MLP Settings\n",
        "  mlp = NeuralHighlighter(depth, 256, 2).to(device)\n",
        "  optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "  # list of possible colors\n",
        "  rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "  color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "  full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "  colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "  # --- Prompt ---\n",
        "  # encode prompt with CLIP\n",
        "  clip_model, preprocess = get_clip_model(clip_model)\n",
        "  with torch.no_grad():\n",
        "          prompt_token = clip.tokenize([prompt]).to(device)\n",
        "          encoded_text = clip_model.encode_text(prompt_token)\n",
        "          encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "  vertices = copy.deepcopy(mesh.vertices)\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  # Optimization loop\n",
        "  for i in tqdm(range(n_iter)):\n",
        "      optim.zero_grad()\n",
        "\n",
        "      # predict highlight probabilities\n",
        "      pred_class = mlp(vertices)\n",
        "\n",
        "      # color and render mesh\n",
        "      sampled_mesh = mesh\n",
        "      color_mesh(pred_class, sampled_mesh, colors)\n",
        "      rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                              show=False,\n",
        "                                                              center_azim=0,\n",
        "                                                              center_elev=0,\n",
        "                                                              std=1,\n",
        "                                                              return_views=True,\n",
        "                                                              lighting=True,\n",
        "                                                              background=background)\n",
        "\n",
        "      # Calculate CLIP Loss\n",
        "      loss = clip_loss(rendered_images, encoded_text, clip_model, n_augs)\n",
        "      loss.backward(retain_graph=True)\n",
        "\n",
        "      optim.step()\n",
        "\n",
        "      # update variables + record loss\n",
        "      with torch.no_grad():\n",
        "          losses.append(loss.item())\n",
        "\n",
        "      # report results\n",
        "      if i % 100 == 0:\n",
        "          print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "          save_renders(log_dir, i, rendered_images)\n",
        "          with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
        "              f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "\n",
        "  # save results\n",
        "  save_final_results(log_dir, objbase, mesh, mlp, vertices, colors, render, background)\n",
        "\n",
        "  # Save prompts\n",
        "  with open(os.path.join(log_dir, prompt), \"w\") as f:\n",
        "      f.write('')\n",
        "\n",
        "  #Exploration directories\n",
        "  file_name = str(learning_rate) + '_' + str(depth) + '_' + str(n_augs) + '_' + str(n_views) + '_' + str(losses[-1])\n",
        "  exploration_dir = os.path.join(output_dir, objbase)\n",
        "  Path(exploration_dir).mkdir(parents=True, exist_ok=True)\n",
        "  save_final_results(exploration_dir, file_name, mesh, mlp, vertices, colors, render, background)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hypterparameter exploration\n",
        "objects = {\n",
        "    'data/horse.obj' : 'a gray horse with highlighted shoes.'\n",
        "}\n",
        "learning_rates = [0.001, 0.0001, 0.00001]\n",
        "depths = [2, 4, 6]\n",
        "n_augss = [2, 5, 8]\n",
        "n_viewss = [2, 5, 8]\n",
        "\n",
        "#learning_rate\n",
        "for obj_path in objects:\n",
        "  prompt = objects[obj_path]\n",
        "  for learning_rate in learning_rates:\n",
        "    optimize(obj_path, prompt, learning_rate, 4, 5, 5)\n",
        "\n",
        "#depth\n",
        "for obj_path in objects:\n",
        "  prompt = objects[obj_path]\n",
        "  for depth in depths:\n",
        "    optimize(obj_path, prompt, 0.0001, depth, 5, 5)\n",
        "\n",
        "#n_augs\n",
        "for obj_path in objects:\n",
        "  prompt = objects[obj_path]\n",
        "  for n_augs in n_augss:\n",
        "    optimize(obj_path, prompt, 0.0001, 4, n_augs, 5)\n",
        "\n",
        "#n_views\n",
        "for obj_path in objects:\n",
        "  prompt = objects[obj_path]\n",
        "  for n_views in n_viewss:\n",
        "    optimize(obj_path, prompt, 0.0001, 4, 5, n_views)\n"
      ],
      "metadata": {
        "id": "NHTCkrdpHtok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Hyperparamter testing\n",
        "objects = {\n",
        "    'data/candle.obj' : 'a gray candle with highlighted hat.',\n",
        "    'data/dog.obj' : 'a gray dog with highlighted hat.'\n",
        "}\n",
        "learning_rates = [0.001, 0.0001, 0.00001]\n",
        "depths = [2, 4, 6]\n",
        "n_augss = [2, 5, 8]\n",
        "n_viewss = [2, 5, 8]\n",
        "\n",
        "#learning_rate\n",
        "for obj_path in objects:\n",
        "  prompt = objects[obj_path]\n",
        "  for learning_rate in learning_rates:\n",
        "    optimize(obj_path, prompt, learning_rate, 4, 5, 5)\n",
        "\n",
        "#depth\n",
        "for obj_path in objects:\n",
        "  prompt = objects[obj_path]\n",
        "  for depth in depths:\n",
        "    optimize(obj_path, prompt, 0.0001, depth, 5, 5)\n",
        "\n",
        "#n_augs\n",
        "for obj_path in objects:\n",
        "  prompt = objects[obj_path]\n",
        "  for n_augs in n_augss:\n",
        "    optimize(obj_path, prompt, 0.0001, 4, n_augs, 5)\n",
        "\n",
        "#n_views\n",
        "for obj_path in objects:\n",
        "  prompt = objects[obj_path]\n",
        "  for n_views in n_viewss:\n",
        "    optimize(obj_path, prompt, 0.0001, 4, 5, n_views)\n"
      ],
      "metadata": {
        "id": "_HkDSY0tZ1wE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}